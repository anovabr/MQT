# Regressão linear simples


```{r, include = FALSE }
load("~/anovabr/mqt/bases/Base R - imagem corporal.RData")

library(tidyverse)
library(olsrr) #regression diagnostics
library(pander)
```

De forma geral, modelos de regressão são modelos estatísticos que visam predizer o comportamento de uma variável dependente (Y) como uma função de uma ou mais variáveis independentes (X). Em larga escala, eles substituem os outros testes paramétricos vistos até agora. Dessa maneira, quase tudo o que foi visto durante os capítulos anteriores são casos especiais de modelos de regressão [@Chartier2008]. 

Existem diferentes nomenclaturas utilizadas para classificar tais modelos e a tabela abaixo apresenta uma classificação funcional.

  | VI e VD      | VD Discreta    |  VD Contínua                 |  
  | :----------- | :-----------   | :-----------                 |        
  | VI Discreta  | Reg. logística |  Reg. linear (Teste T/ANOVA) |   
  | VI Contínua  | Reg. logística |  Reg. linear                 | 
  

Algumas conclusões são possíveis:  

1. A variável dependente irá definir se a regressão será linear ou logística. Quando a VD é continua (ex: peso, tempo de resposta, inteligência) trata-se de uma regressão linear. Quando a VD é discreta ou categórica (ex: acidente - sim ou não; orientação política - direita ou esquerda) trata-se de uma regressão logístic.    
2.  Caso haja uma única VI, a regressão é chamada de simples. Com duas ou mais VIs, ela é chamada de múltipla.  
3.  Se houver mais de uma VD, o modelo será chamado de multivariado (em inglês, path analysis.  
4.  Teste T e ANOVA são casos de regressão linear simples     
5.  ANCOVA, ANOVA de k vias e ANOVA fatorial são casos de regressão múltipla  
6.  O qui-quadrado pode ser aproximado pela regressão logística simples e vice-versa  


Isso posto, a Regressão linear é uma técnica estatística que permite estimar o quanto os valores de uma variável dependente (Y) variam em função de uma ou mais variáveis independentes (X). Isso é feito através de uma equação específica e há, ao menos, duas utilidades diretas em uma pesquisa, que são:  

(i) <mark>Predizer</marK> os valores da variável dependente (Y) em função dos valores da variável dependente (X);  
(ii) <mark>Explicar</marK> a variabilidade da variável dependente (Y) em função da variável independente (X). 

Abas as utilidades são virtualmente iguais e como a Regressão linear simples pode ser vista a partir de um incremento ou avanço dos modelos de correlação, os aspectos correlacionais devem (e podem) ser inicialmente investigados.  


Pela abrangência dos modelos de regressão, é possível tanto encontrar cursos completos e detahados sobre seus detalhes, como abordagens mais pragmáticas e operacionais voltadas a implementação deles em pesquisas. Nesse capítulo, o foco será dado na capacidade operacional.

Conceitualmente, a regressão linear simples é apresentada como:

\[y_i = b_0 + b_1X{_1}_i + \epsilon_{i}\]

$y_i$ representa a variável dependente  
$b_0$ é o intercepto (coeficiente linear)   
$b_1$ é a inclinação (coeficiente angular)  
$\epsilon_{i}$ é o erro/resíduo   


A interpretação dos resultados obtidos depende dos seguintes pressupostos:  
 
(i) A relação entre as variáveis é linear
(ii) Os resíduos são independentes  
(ii) Os resíduos são normalmente distribuídos, com média 0 e variância constante  
(iii) A variância dos resíduos é constante  

O mnemônico <u>LINE</u> talvez ajude a lembrar destes pressupostos. Ele se refere à  <u>linearity, independence, normality, equal variance </u>.  


## Glossário        
 
Diferentes termos são empregados em modelos de regressão. Alguns deles são apresentados a seguir para auxiliar no entendimento e também aproximar o leitor a este tipo de modelagem.    

```{block, type="writing"}
**intercepto** ($b_0$): Valor previsto (médio) de Y quando X = 0  
**Inclinação** ($b_i$): Diferença média em unidades da variável dependente quando se altera uma unidade de X   
**SSR**: Soma dos Quadrados da Regressão   
**SSE**: Soma dos Quadrados dos Erros  
**SST**: Soma dos Quadrados Total  
**Coeficiente de Determinação** ($R^2$): Porcentagem de variação da variável dependente (Y) que pode ser atribuída à variabilida da(s) variável (is) independente(s) (X)  
**Coeficiente de Determinação ajustado** ($R^2_{adj}$): Coeficiente que pondera o $R^2$ pelo número de variáveis explicativas e pelo número de observações da amostra. É particularmente útil quando deseja-se comparar modelos de regressão múltipla para mesma variável dependente, pois penaliza aquele modelo com maior número de variáveis independentes  
RMSEA ($Res. St. Error$): Desvio padrão dos valores previstos da variável dependente ao redor da linha de regressão estimada   
```

O conhecimento de algumas fórmulas fechadas também auxilia no entendimento da modelagem.

```{block, type="writing"}
Soma dos Quadrados da Regressão: $SSR =  \sum_{i=1}^{n}(\hat{y} - \bar{y})^2$  

Soma dos Quadrados dos Erros: $SSE =  \sum_{i=1}^{n}(y_i - \hat{y})^2$  

Soma dos Quadrados Total: $SST =  \sum_{i=1}^{n}(y_i - \bar{y})^2$  

Variabilidade total: $SST = SSR + SSE$  

$R^2$:  $\frac{SSR}{SST} = 1- \frac{SSE}{SST}$  

Erro quadrático médio: $MSE =  \sum_{i=1}^{n}(y_i - \hat{y})^2 /(N-K)$  

$R^2_{adj}$: $1-\frac{MSE}{MSR}$  

$Res. St. Error = \sqrt\frac{SSE}{N-K}$  
```




## Breve explicação conceitual


Frequentemente, os livros tendem a explicar modelos de regressão pela apresentação de conjuntos, gráficos de dispersão ou conceitos analíticos/matemático.  Neste capítulo, a primeira explicação será introduzida.   

Inicialmente, é necessário notar que a <u>variável dependente (Y)</u> e a <u>variável independente (X)</u> podem ser vistas como conjuntos independentes. No caso:


![](./img/cap_reg_x_y.png)


Neste exemplo, ambas as variáveis estão afastadas e não há nenhuma relação entre elas. No entanto, o que frequentemente ocorre é que existe algum grau de relacionamento entre as variáveis, tal como exposto abaixo.  

![](./img/cap_reg_x_y2.png)


Caso se assuma que X é um fator de causalidade à realização de Y, isso significa que uma parte da realização de Y, necessariamente, depende de X. Essa é a área de intersecção destacada e deve ser entendida como a parte de Y que pode ser atribuída ou explicada por X. 

Analiticamente, essa área precisa de algumas transformações algébricas e, em função delas, recebe o nome de `Soma dos Quadrados da Regressão` (SSR, em inglês).  

![](./img/cap_reg_x_y_SSR.png)


No entanto, nem toda a variabilidade de Y pode ser atribuída à X. Essa região fora de Y fora da intersecção também sofre algumas transformações algébricas e recebe o nome de `Soma dos Quadrados dos Erros` (SSE, em inglês).  

Essa área representa a variabilidade de Y que não pode ser atribuída/explicada por X. Nesse caso: 

![](./img/cap_reg_x_y_SSE.png)


Agora, independentemente de X, é possível verificar que Y possui (internamente) uma variabilidade total. Essa variabilidade é o somatório da área explicada pela regressão (SSR) com a área não explicada (SSE). Essa região total também passará por transformações algébricas e é chamada de `Soma dos Quadrados Total` (SST, em inglês).


![](./img/cap_reg_x_y_SST.png)


Vendo todas as partições de uma única vez, temos o seguinte:


![](./img/cap_reg_x_y_SSE_SSR_SST.png)


A porcentagem de variação de Y que pode ser atribuída à variabilidade de X é uma razão entre a Soma dos Quadrados da Regressão (SSR) e a Soma dos Quadrados Total (SST). O coeficiente obtido por essa razão recebe o nome de **Coeficiente de Determinação** ou $R^2$, apresentado inicialmente.

![](./img/cap_reg_r2.png) 

Isso é equivalente a subtração do espaço máximo de variabilidade (1 ou 100%) pela razão entre a Soma dos Quadrados dos Erros (SSE) e Soma dos Quadrados Total (SST):


![](./img/cap_reg_r21.png)

Evidentemente, essa explicação é apenas inicial e serve apenas para introduzir as principais ideia da modelagem de regressão de uma maneira intuitiva. Existem excelente obras mais detalhadas e com aplicações à Psicologia, entre elas:  
Data Analysis: A Model Comparison Approach To Regression, ANOVA, and Beyond, de Charles M. Judd,  Gary H. McClelland e Carey S. Ryan   

Regression, ANOVA, and the General Linear Model": A Statistics Primer, de Paul Vik  

Regression and Other Stories, de Andrew Gelman  

## Pesquisa  

<div class="alert alert-info" role="alert">
  <strong>Base: </strong> Livro - Dados - Eating disorders
</div>  


Neste capítulo, vamos utilizar a pesquisa intitulada ["Aspects Related to Body Image and Eating Behaviors in Healthy Brazilian Undergraduate Students"](https://www.researchgate.net/publication/323729370_Aspects_Related_to_Body_Image_and_Eating_Behaviors_in_Healthy_Brazilian_Undergraduate_Students), publicada em 2018 no Global Journal of Educational Studies, que sou coautor.

O objetivo dessa pesquisa foi explorar os fatores envolvidos em transtornos alimentares e na percepção da imagem corporal. Os primeiros aspectos foram avaliados pela escala EAT-26, enquanto o segundo foi avaliado pela escala BSQ-34.  

Uma das principais hipóteses era que alterações na percepção da imagem corporal seriam preditores para possíveis transtornos alimentares. Operacionalmente, a hipótese era de que os valores da BSQ-34 poderiam predizer os valores da EAT-26 e que quão maior fossem os primeiros, maior seriam os efeitos na maximização dos segundos.    

Em modelos de regressão, as hipóteses costumam ser feitas em cascata. Quase sempre, se compara o modelo de desenvolvido com um modelo mais simples. Em seguida, verifica-se cada preditor de forma individual e assim sucessivamente. Uma vez que a definição de cada hipótese ocuparia um espaço grande aqui, elas serão suprimidas.  



## Execução no R  


A primeira etapa da análise é realizada pelo desenvolvimento de tabelas e gráficos que possam auxiliar na interpretação dos resultados. De maneira similar à feita em outros capítulos, abaixo há uma tabela descritiva . 

```{r results="asis" }
arsenal::tableby(~eat_soma + bsq_soma, dados_brasil) %>% summary() 
```



O cálculo da correlação entre ambas as variáveis também é importante e, apesar de tecnicamente desnecessário neste capítulo, os resultados apresentam evidências iniciais sobre o relacionamento bivariado. Em linhas gerais, o coeficiente de correlação expressa a força e a direção do relacionamento entre as variáveis. A força pode ser interpretada como `fraca` (0.1), `moderada` (0.3) ou `forte` (0.5) [@Cohen1988] e a direção pode ser positiva ou negativa, a depender do sinal. A correlação entre as variáveis foi 0.675 e significativa (p < 0.001). 

```{r}
cor.test(dados_brasil$eat_soma, dados_brasil$bsq_soma) %>% pander()
```
   
O gráfico de dispersão descreve esse relacionamento. No eixo X deve-se inserir a VI (neste caso, os resultados da BSQ-34), enquanto a VD é inserida em Y.

```{r}
ggplot(dados_brasil, aes(x = bsq_soma, y = eat_soma)) +
  geom_jitter()
```

Tanto a tabela como o gráfico deixam claro que existe um padrão (aproximadamente linear) entre ambas as variáveis que ocorre de maneira forte e significativa. Com isso, é natural que o interesse seja verificar o quanto os resultados do <u>EAT-26</u> variam em função do <u>BSQ-34</u>.   

Para executar esse procedimento, o R conta com a função nativa `lm`. Por sua vez, o pacote `olsrr` oferece excelentes complementos para interpretar os achados.  O vetor `mod_linear_simples` será criado e armazenará os resultados. Lembre-se que, no R, é importante <u>sempre</u> atentar para o nível de medida das variáveis para que os resultados sejam adequados.  


```{r}
mod_linear_simples <- lm(eat_soma ~ bsq_soma, data = dados_brasil)
```


Na maioria dos programas comerciais, os resultados do modelo de regressão são apresentados em uma tabela padronizada. Essa tabela é virtualmente identica à que foi exposta no capítulo sobre a ANOVA de uma via e encontra-se abaixo descrita:    

  | Fonte de variação | SS               | df  | MS     | F-Value        | P-Value  |  
  | :-----            | :-----           | :-- | :----- | :-----         | :-----   |   
  | Regressão         | SSR (Regressão)  | K-1 | MSR    | SSR/K-1        | MSR/MSE  |   
  | Erro              | SSE (Erro)       | N-K | MSE    | SSE/N-K        | --       |     
  | Total             | SST (Total)      | N-1 | --     |  --            | --       |       
  | R2 = SSR/SST                                                                    |   
  
Nota: Nessa tabela, K considera dois preditores na regressão, que são o intercepto e a inclinação. É possível também encontrar `N-K-1` em alguns livros que não explicitam o intercepto na tabela.  

Isto explicado, a função `ols_regress` do pacote `olsrr` dispoõe os resultados neste padrão:  


```{r }
ols_regress(mod_linear_simples)
```

Os resultados apresentados na tabela de regressão são muitos e se recomenda uma ordem específica para interpretá-los.   

Em <u>primeiro momento</u>, é necessário verificar o ajuste do modelo na seção `ANOVA`. Esse resultado compara o modelo em questão contra um modelo em que apenas o intercepto é utilizado para prever todos os valores. Tecnicamente, o modelo analisado é chamado de <u>irrestrito</u> (ou aumentado) e o modelo que tem apenas o intercepto é chamado de <u>restrito</u> ou nulo. Valores significativos são necessários nesta etapa.  Nesta análise, o resultado foi F(1, 218) = 182.883, p < 0.0001, indicando que os outros resultados podem ser interpretados.  

O <u>segundo momento</u> é a interpretação do $R^2$. Como exposto no início do capítulo, essa indicador mensura a parte da variação da variável dependente (Y) que pode ser atribuída às variáveis independentes do modelo (X). Repare que ele é computado pela razão entre o SSR e o SST e indica que cerca de 46% dos resultados da variabilidade do EAT-26 podem ser explicados pelo modelo.     


O <u>terceiro momento </u> é a análise do $R^2 ajustado$. Em modelos de regressão, modelos com mais parâmetros/preditores sempre vão sempre ter $R^2$ maior do que modelos mais compactos, independente da signficância destes outros parâmetros. O $R^2 ajustado$ é uma medida que considera a complexidade do modelo e pune a entrada de novas variáveis. Neste caso, como há apenas dois preditores (intercepto e bsq_soma), o $R^2 ajustado$ e o $R^2$ são quase idênticos.  

Finalmente, o <u>quarto momento</u> é análise dos preditores, que é feito na seção `Parameter Estimates`. Para isso, deve-se identificar os preditores um a um, seus valores de `Beta` e de P (`Sig`). O `Beta` indica a diferença média em unidades da <u>variável dependente</u> quando se altera uma unidade de X. Por exemplo, mais 1 ponto no BSQ-34, mais `0.178` pontos, em média, no EAT-26. Esse resultado é significativo, tal como é indicado na coluna `Sig`.  

O intercepto é chamado de `constante` na maior parte dos programas e indica o valor médio (esperado) de Y quando `X=0`. Nesse caso, se alguém tivesse tirado o valor `0` na escala BSQ-34, o valor previsto para os resultados da Escala EAT-26 seria de 1.46. O `Sig` indica que esse valor é significativo, ou seja, diferente de 0. O indicador de beta padronizado `Std. Beta` traz as mesmas informações, mas trabalha em unidades de desvios-padrão em todas as variáveis presentes no modelo.  

É importante notar que frequentemente o intercepto não tem interpretação lógica e, por isso, costuma ser desconsiderado. Para que ele tenha melhor capacidade de interpretação e se torne o valor médio da variável dependente, é necessário centralizar os valores do preditor $(x_i-\bar{x})$, que não será realizado aqui.   


Essa interpretação é muito auxiliada na apresentação de gráficos de dispersão, tal como feito no início do capítulo. Entretanto, agora estes gráficos ganham dois elementos a mais: uma reta de regressão que irá indicar o intercepto, a inclinação e o intervalo de confiança das estimativas e uma indicação textual com as equações características e seus respectivos resultados. Essas adições gráficas são feitas pelo pacote `ggpubr`. 

```{r}
ggplot(dados_brasil, aes(x = bsq_soma, y = eat_soma)) +
  geom_jitter() + geom_smooth(method = "lm") + 
  ggpubr::stat_regline_equation(label.x = 3, label.y = 40) +
  ggpubr::stat_cor(method = "pearson", label.x = 3, label.y = 44)
```


Uma vez que o modelo já foi realizado, a interpretação dos resultados depende da adequação de seus pressupostos. A violação destes pressupostos distorce, limita ou invalida as interpretações teóricas propostas, uma vez que tanto o aumento do erro do tipo 1 (falso positivo), como do tipo 2 (falso negativo) podem ocorrer [@Lix1996; @Barker2015; @Ernst2017]. Corriqueiramente, testar os pressupostos é uma etapa <u>anterior</u> à própria realização do teste inferencial. Entretanto, <u>pedagogicamente</u> a apresentação deles após a execução do teste parece mais adequada. Assim, eles serão testados a seguir.

<mark> Normalidade</mark>: O pressuposto da Normalidade é atendido se os <u>resíduos</u> do modelo de regressão seguirem uma distribuição normal. Isso pode ser avaliado graficamente por QQ plots e também por testes específicos, como o Shapiro-wilk, Anderson-Darling e Jarque Bera.

O QQ plot é um gráfico que reúne a distribuição empírica ordenada dos quantis contra os quantis da distribuição teórica (aqui, normal). Se os dados e a linha diagonal se soprepuserem, isso é uma evidencia de que a distribuição empírica é a mesma da distribuição teórica. Caso haja discrepância, isso aponta para desvio da normalidade.  Caso os pontos e a reta diagonal estejam superpostos, se considera que este pressuposto foi atendido. 

```{r}
ols_plot_resid_qq(mod_linear_simples)
```

Testes estatísticos formais também podem ser utilizados, tal como abaixo:  

```{r}
ols_test_normality(mod_linear_simples)
```
Apesar dos resultados obtidos por tais testes serem algo discordantes, os achados sugerem violação deste pressuposto.  

<mark>Independência</mark>: A independência dos resíduos depende bastante do delineamento utilizado ser transversal ou longitudinal. O teste de Durbin Watson pode ser utilizado e a hipótees nula é de que os resíduos não são correlacionados. Este pressuposto foi atendido, o que já era esperado.  

```{r}
car::durbinWatsonTest(mod_linear_simples)
```

<mark>Homocedasticidade</mark>: Este pressuposto de variâncias constantes pode ser analisada em um gráfico de dispersão dos resíduos (residual) contra os valores previstos (fitted).  

```{r}
ols_plot_resid_fit(mod_linear_simples)
```

Caso haja padrões neste gráfico, isso sugere que este pressuposto foi violado. O gráfico não sugere padrões específicos. No entanto, testes formais são recomendados para que a decisão tomada tenha maior apoio. Existem diferentes testes para isso e, entre eles, o teste de Bartlett, Levene e Breusch-Pagan. Os resultados dependem das propriedades de cada um dos modelos e, em função da praticidade computacional, o teste de Breusch-Pagan será utilizado. Em todos estes testes, a hipótese nula assume homocedasticidade. Portanto, a estatística de teste não deveria ser significativa para que a homocedasticidade fosse apoiada.


```{r}
ols_test_breusch_pagan(mod_linear_simples)
```
Os resultados indicaram que a homocedasticidade foi violada. Isso vai na direção oposta da percepção gráfica, o que pode ocorrer sem nenhum problema.  


Isso posto, os diagnósticos executados indicaram que o modelo violou a normalidade e a homocedasticidade e preservou a linearidade e a independência dos resíduos. Apesar desse tipo de resultado ser frequente em Psicologia, a interpretação dos resultados é limitada e deve ser feita de forma apenas preliminar. Abaixo uma orientação de como escrever os resultados.

## Tamanho do efeito  

Em modelos de regressão, o valor de 


uma vez que alguns achados da literatura comentam que a <u>alterações da alimentação</u> ocorrem em função da percepção da <u>imagem corporal</u>. Dando a este objetivo uma leitura estatística, o interesse é o de prever os valores do <u>EAT-26</u> a partir dos valores do <u>BSQ-34</u> ou o quanto a variabilidade dos resultados do <u>EAT-26</u> pode ser atribuída pelos resultados do <u>BSQ-34</u>. 

Fazer isso pede que se retorne ao gráfico correlacional feito ainda pouco e que tente se ajustar / traçar uma reta que tente tocar na maioria dos pontos. Milhões de retas podem ser traçadas e todas acertarão alguns pontos, mas errarão outros. Por exemplo:

```{r}
ggplot(dados_brasil, aes(x = bsq_soma, y = eat_soma)) +
  geom_jitter() +
  labs(x = "Resultados da Escala BSQ-34", y = "Resultados da Escala EAT-26", 
       title = "Correlação entre o BSQ-34 e o EAT-26") +
  xlim(10,200) +
  geom_abline(slope = c(rnorm(10,0.4,0.6), rnorm(10,0.2,0.2)),color = 1:20)

```

A necessidade agora é conseguir encontrar o <u>melhor</u> modelo estatístico que descreva essa relação. Assim, obter uma função que possa gerar uma reta que esteja bem perto dos pontos reais e, consequentemente, minimize os erros. Isso é feito justamente resgatando o conceito de função afim, exposto no ensino médio (e ilustrado ao início do capítulo):


$$\hat{y} = a + bX$$

Repare que agora o valor previsto ($\hat{y}$) depende de duas constantes (<u>a: intercepto ou coeficiente linear</u> e <u>b: inclinação ou coeficiente angular</u>) e uma variável (X). Apenas por uma questão de simbologia, três alterações são feitas com a equação:    

(i) Os símbolos são alterados. Agora $a = b_0$ e $b = b_1$. A alteração de simbologia não altera em nada os cálculos.  

(ii) Como se sabe que essa reta vai <u>estimar</u> os valores reais de $Y$, letras minúsculas ou um chapéu sobre as letras será utilizado em vez das letras maiúsculas ou gregas.  

(iii) Para que cada valor estimado seja associado a um participante a letra $i$ será adicionada abaixo do $y$ e do $b_1$.   

Assim, temos que os valores estimados de y, agora $\hat{y}$, são obtidos pelo $b_0$ e $b_1$:

$$\hat{y}_i = b_0 + b_1X{_1}_i$$


A equação visa minimizar os erros e não anulá-los. Oy seja, entre o valor real de y (os pontos que estão no gráfico) e os valores obtidos minha equação, haverá <u>sempre</u> uma certa quantidade de erro de estimativa ($e_i$). Qualquer que seja o valor estimado de cada participante ($i$), sempre haverá uma quantidade de erro ($e_i$). Dessa forma, é possível descrever que os valores reais possuem uma porção de erro:

$$y_i = a + b_1X{_1}_i+\underbrace{e_i}_\text{aleatório}$$

uma vez que é possível traçar milhões de retas, para encontrar a reta que minimize os erros será necessário discriciona-lo.

$$e_i = y_i - \hat{y_i}$$
$$e_i = y_i - (b_0 + b_1X{_1}_i) \\ =  y_i - b_0 - b_1X{_1}_i$$


O método mais frequentemente utilizado para minimizar os erros é o Mínimos Quadrados Ordinários (em inglês, Ordinary Least Squares -- OLS). Isso é realizado minimizando a soma dos quadrados das diferenças entre os valores estimados de Y por meio
da reta de regressão ($\hat{Y}$). Para isso, o procedimento consiste em derivar a soma dos quadrados dos erros em relação a $b_0$ e e $b_1$ e, em seguida, igualando a 0:

$$\frac{\partial \epsilon}{\partial b_0}  = 0,\\ \frac{\partial \epsilon}{\partial b_1} = 0$$

Com isso, os resultados permitem concluir que a inclinação da reta (slope) é dada por:

$$\begin{aligned}
b_1 &= \frac{\sum_{i = 1}^{n} x_i y_i - \frac{(\sum_{i = 1}^{n} x_i)(\sum_{i = 1}^{n} y_i)}{n}}{\sum_{i = 1}^{n} x_i^2 - \frac{(\sum_{i = 1}^{n} x_i)^2}{n}} = \frac{COV(xy)}{VAR(x)}\end{aligned}$$  

Enquanto o intercepto é dado por:

$$\begin{aligned}b_0 &= \bar{y} - b_1 \bar{x}
\end{aligned}$$

Agora é possível traçar a <u>melhor</u> reta para descrever o relacionamento entre as variáveis, tal como abaixo:


```{r}
b1 <- cov(dados_brasil$bsq_soma, dados_brasil$eat_soma)/var(dados_brasil$bsq_soma)
b0 <- mean(dados_brasil$eat_soma)-(b1*mean(dados_brasil$bsq_soma))

ggplot(dados_brasil, aes(x = bsq_soma, y = eat_soma)) +
  geom_jitter() +
  labs(x = "Resultados da Escala BSQ-34", y = "Resultados da Escala EAT-26", 
       title = "Correlação entre o BSQ-34 e o EAT-26") +
  geom_abline(intercept = b0, slope = b1)
```


Essa reta passará necessariamente pela média de ambas as variáveis.

```{r}
ggplot(dados_brasil, aes(x = bsq_soma, y = eat_soma)) +
  geom_jitter() +
  labs(x = "Resultados da Escala BSQ-34", y = "Resultados da Escala EAT-26", 
       title = "Correlação entre o BSQ-34 e o EAT-26") +
  geom_abline(intercept = b0, slope = b1) +
  geom_vline(xintercept = mean(dados_brasil$bsq_soma), size=1.5, color = "red", linetype = "dashed") +
  geom_hline(yintercept = mean(dados_brasil$eat_soma), size=1.5, color = "red", linetype = "dashed")
```

Aproveitando o gráfico, agora é possível apresentar os dados reais, a linha de regressão e as distâncias (resíduos) entre os pontos reais e os previstos. Enquanto o modelo foi preciso em alguns pontos, em outros ele não se saiu assim tão bem. No entanto, como essa reta foi construída pela minimização da soma dos quadrados dos resíduos, isso nos deixa confortável com os resultados.

```{r}
transform(dados_brasil, Fitted = fitted(mod_linear_simples)) %>% 
  ggplot(., aes(y = bsq_soma, x = eat_soma)) +
  geom_point(aes(y = bsq_soma, x = eat_soma, shape = "real"), color="black") + #plot real
  geom_point(aes(y = Fitted, shape =  "previsto"), color="1") + #plot previsto
  geom_smooth(se=FALSE, method = "lm", color = "black") + 
  geom_segment(aes(x = eat_soma, y = bsq_soma, xend = eat_soma, yend = Fitted), color= "red") + #erro ligado
  scale_colour_manual(name = "Legenda",
                      labels = c("Estimados/Previstos", "Reais dos dados"),
                      values = c("green", "black")) +   
  scale_shape_manual(name = "Legenda",
                     labels = c("Estimados/Previstos", "Reais dos dados"),
                     values = c(1, 5)) +
  labs(x = "Escala BSQ-34", y = "Escala EAT-26", title = "Resultados previstos vs. reais")

```



  


$$Adjusted R^2 = 1 - \frac{SSE/(N-K)}{SST/(N-1)} = 1-\frac{11328.675/(220-2)}{20832.45/(220-1)} = 1-\frac{51.97}{95.13} = 0.454$$

Com tais resultados descritos, agora é possível retornar aos coeficientes obtidos na regressão.


O `intercepto` é chamado de `constante` na maior parte dos programas. Ele se refere ao valor médio (esperado) de Y quando X=0. Ou seja, se alguém tivesse tirado o valor 0 na escala BSQ-34, o valor previsto para os resultados da Escala EAT-26 seria de 1.55, tal como apresentado abaixo:

```{r}
predict(mod_linear_simples, data.frame(bsq_soma=c(0)))
```


Já o coeficiente do `bsq_soma` se refere os resultados obtidos a partir da Escala BSQ-34 é 0.178 e significativo. Isso significa que 1 unidade de mudança nos resultados da BSQ-34 geram 0.177 unidade de mudança, em média, nos resultados da Escala EAT-26. A significância deste coeficiente é dada pelo Erro Padrão (Std. Error) e segue uma T com n-2 graus de liberdade.





```{block, type="writing"}
**Como escrever os resultados**  

Um modelo de regressão foi calculado para verificar os resultados dos comportamentos alimentares (EAT-26) em função da percepção de imagem corporal (BSQ-34). Os resultados indicaram que cerca de 45% da variância do EAT-26 pode ser atribuída ao BSQ-34 (R2 = 0.456, F(1,218) = 182.88, p < 0.001). Cada ponto a mais no BSQ-34 impacta, em média, em 0.178 no EAT-26 (b = 0.178, p < 0.001). 

```  



## Resumo  

1. Existem diferentes modelos de regressão e eles sempre visam prever um resultado a partir de uma ou um conjunto de variáveis  
2. O tipo de modelagem depedente tanto da natureza e quantidade das VIs e VDs, sempre possível entender grande parte dos testes estatísticos estudados como casos particulares dos modelos de regressão  
3. Os principais indicadores de um modelo de regressão são sua significância geral, o $R^2$,o o $R^2_{adj}$, bem como o coeficiente e a significância dos preditores  
4. o diagnóstico é uma parte essencial desta modelagem e o mnemônico LINE pode ajudar na lembrança dos pressupostos  
